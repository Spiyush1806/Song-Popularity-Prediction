{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a89e3ab3",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86b72122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Optional: import LGBM and XGB if available\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except ImportError:\n",
    "    LGBMClassifier = None\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ImportError:\n",
    "    XGBClassifier = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096b318",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11117d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your working directory\n",
    "os.chdir('/home/piyush/umc301/iisc-umc-301-kaggle-competition-1')\n",
    "\n",
    "# Read train and test data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691824c6",
   "metadata": {},
   "source": [
    "Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "310dcf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df = df.copy()\n",
    "    # Basic features\n",
    "    if \"song_duration_ms\" in df.columns:\n",
    "        df[\"duration_min\"] = df[\"song_duration_ms\"] / 60000.0\n",
    "        df[\"log_duration\"] = np.log1p(df[\"song_duration_ms\"])\n",
    "    if \"energy\" in df.columns and \"danceability\" in df.columns:\n",
    "        df[\"energy_dance_ratio\"] = df[\"energy\"] / (df[\"danceability\"] + 1e-6)\n",
    "        df[\"energy_plus_dance\"] = df[\"energy\"] + df[\"danceability\"]\n",
    "        df[\"energy_times_dance\"] = df[\"energy\"] * df[\"danceability\"]\n",
    "    if \"loudness\" in df.columns and \"energy\" in df.columns:\n",
    "        df[\"loudness_energy_ratio\"] = df[\"loudness\"] / (df[\"energy\"] + 1e-6)\n",
    "        df[\"loudness_times_energy\"] = df[\"loudness\"] * df[\"energy\"]\n",
    "    if \"audio_valence\" in df.columns and \"energy\" in df.columns:\n",
    "        df[\"valence_energy_ratio\"] = df[\"audio_valence\"] / (df[\"energy\"] + 1e-6)\n",
    "    if \"tempo\" in df.columns and \"time_signature\" in df.columns:\n",
    "        df[\"tempo_per_measure\"] = df[\"tempo\"] / (df[\"time_signature\"].replace(0, 1))\n",
    "    # Polynomial features\n",
    "    for col in [\"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \"liveness\"]:\n",
    "        if col in df.columns:\n",
    "            df[f\"log_{col}\"] = np.log1p(df[col])\n",
    "            df[f\"{col}_squared\"] = df[col] ** 2\n",
    "    # Binning\n",
    "    if \"loudness\" in df.columns:\n",
    "        df[\"loudness_bin\"] = pd.cut(df[\"loudness\"], bins=5, labels=False)\n",
    "    if \"tempo\" in df.columns:\n",
    "        df[\"tempo_bin\"] = pd.cut(df[\"tempo\"], bins=5, labels=False)\n",
    "    if \"energy\" in df.columns:\n",
    "        df[\"energy_bin\"] = pd.cut(df[\"energy\"], bins=5, labels=False)\n",
    "    if \"danceability\" in df.columns:\n",
    "        df[\"danceability_bin\"] = pd.cut(df[\"danceability\"], bins=5, labels=False)\n",
    "    # Extra interactions\n",
    "    if \"energy\" in df.columns and \"loudness\" in df.columns and \"danceability\" in df.columns:\n",
    "        df[\"energy_loudness_dance\"] = df[\"energy\"] * df[\"loudness\"] * df[\"danceability\"]\n",
    "    if \"acousticness\" in df.columns and \"instrumentalness\" in df.columns:\n",
    "        df[\"acoustic_instrumental\"] = df[\"acousticness\"] * df[\"instrumentalness\"]\n",
    "    if \"liveness\" in df.columns and \"speechiness\" in df.columns:\n",
    "        df[\"liveness_speech\"] = df[\"liveness\"] * df[\"speechiness\"]\n",
    "    # Treat key, audio_mode, time_signature as categorical if present\n",
    "    for col in [\"key\", \"audio_mode\", \"time_signature\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228d2a7",
   "metadata": {},
   "source": [
    "Apply Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96b56572",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = add_features(train)\n",
    "test = add_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6393111",
   "metadata": {},
   "source": [
    "Prepare Data for Modeling and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75f8b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = train['id']\n",
    "test_id = test['id']\n",
    "y = train['song_popularity']\n",
    "\n",
    "X_train = train.drop(['id', 'song_popularity'], axis=1)\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f80e19",
   "metadata": {},
   "source": [
    "Impute Missing Values using MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fe29b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(random_state=42, initial_strategy='mean', max_iter=10)\n",
    "X_full = pd.concat([X_train, X_test], axis=0)\n",
    "X_full_imputed = imputer.fit_transform(X_full)\n",
    "X_train_imputed = X_full_imputed[:len(X_train)]\n",
    "X_test_imputed = X_full_imputed[len(X_train):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab23145",
   "metadata": {},
   "source": [
    "Model Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44b364cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8746, number of negative: 15254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004766 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9186\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364417 -> initscore=-0.556245\n",
      "[LightGBM] [Info] Start training from score -0.556245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [13:43:41] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8746, number of negative: 15254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9185\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364417 -> initscore=-0.556245\n",
      "[LightGBM] [Info] Start training from score -0.556245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [13:44:35] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8746, number of negative: 15254\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003239 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9185\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364417 -> initscore=-0.556245\n",
      "[LightGBM] [Info] Start training from score -0.556245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [13:45:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8745, number of negative: 15255\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9186\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364375 -> initscore=-0.556425\n",
      "[LightGBM] [Info] Start training from score -0.556425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [13:46:16] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8745, number of negative: 15255\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9185\n",
      "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.364375 -> initscore=-0.556425\n",
      "[LightGBM] [Info] Start training from score -0.556425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/piyush/anaconda3/envs/umc203/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [13:47:06] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "NFOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "test_preds_hgb = np.zeros(len(X_test_imputed))\n",
    "test_preds_lgb = np.zeros(len(X_test_imputed))\n",
    "test_preds_xgb = np.zeros(len(X_test_imputed))\n",
    "oof_hgb = np.zeros(len(X_train_imputed))\n",
    "oof_lgb = np.zeros(len(X_train_imputed))\n",
    "oof_xgb = np.zeros(len(X_train_imputed))\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train_imputed, y)):\n",
    "    X_tr, X_val = X_train_imputed[tr_idx], X_train_imputed[val_idx]\n",
    "    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "    # HistGradientBoosting\n",
    "    clf_hgb = HistGradientBoostingClassifier(random_state=42, max_iter=1200, learning_rate=0.01, max_leaf_nodes=63)\n",
    "    clf_hgb.fit(X_tr, y_tr)\n",
    "    oof_hgb[val_idx] = clf_hgb.predict_proba(X_val)[:,1]\n",
    "    test_preds_hgb += clf_hgb.predict_proba(X_test_imputed)[:,1] / NFOLDS\n",
    "    # LightGBM\n",
    "    if LGBMClassifier is not None:\n",
    "        clf_lgb = LGBMClassifier(n_estimators=1200, learning_rate=0.01, max_depth=11, subsample=0.8, colsample_bytree=0.8, reg_alpha=1.0, reg_lambda=1.0, random_state=42)\n",
    "        clf_lgb.fit(X_tr, y_tr)\n",
    "        oof_lgb[val_idx] = clf_lgb.predict_proba(X_val)[:,1]\n",
    "        test_preds_lgb += clf_lgb.predict_proba(X_test_imputed)[:,1] / NFOLDS\n",
    "    # XGBoost\n",
    "    if XGBClassifier is not None:\n",
    "        clf_xgb = XGBClassifier(n_estimators=1200, learning_rate=0.01, max_depth=11, subsample=0.8, colsample_bytree=0.8, reg_alpha=1.0, reg_lambda=1.0, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "        clf_xgb.fit(X_tr, y_tr)\n",
    "        oof_xgb[val_idx] = clf_xgb.predict_proba(X_val)[:,1]\n",
    "        test_preds_xgb += clf_xgb.predict_proba(X_test_imputed)[:,1] / NFOLDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c16211",
   "metadata": {},
   "source": [
    "Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c960e1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC HGB: 0.5567289610983427\n",
      "CV AUC LGB: 0.5596839571833769\n",
      "CV AUC XGB: 0.5486616001038055\n"
     ]
    }
   ],
   "source": [
    "print(\"CV AUC HGB:\", roc_auc_score(y, oof_hgb))\n",
    "if LGBMClassifier is not None:\n",
    "    print(\"CV AUC LGB:\", roc_auc_score(y, oof_lgb))\n",
    "if XGBClassifier is not None:\n",
    "    print(\"CV AUC XGB:\", roc_auc_score(y, oof_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d392599",
   "metadata": {},
   "source": [
    "Weighted Ensemble (Based on AUC Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca26c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "auc_hgb = roc_auc_score(y, oof_hgb)\n",
    "weights.append(auc_hgb)\n",
    "ensemble = test_preds_hgb * auc_hgb\n",
    "if LGBMClassifier is not None:\n",
    "    auc_lgb = roc_auc_score(y, oof_lgb)\n",
    "    weights.append(auc_lgb)\n",
    "    ensemble += test_preds_lgb * auc_lgb\n",
    "if XGBClassifier is not None:\n",
    "    auc_xgb = roc_auc_score(y, oof_xgb)\n",
    "    weights.append(auc_xgb)\n",
    "    ensemble += test_preds_xgb * auc_xgb\n",
    "ensemble /= sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f601055",
   "metadata": {},
   "source": [
    "Stacking (meta-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f59676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (LGBMClassifier is not None) and (XGBClassifier is not None):\n",
    "    stack_X = np.vstack([oof_hgb, oof_lgb, oof_xgb]).T\n",
    "    stack_X_test = np.vstack([test_preds_hgb, test_preds_lgb, test_preds_xgb]).T\n",
    "    meta = LogisticRegression(max_iter=1000)\n",
    "    meta.fit(stack_X, y)\n",
    "    stacked_preds = meta.predict_proba(stack_X_test)[:,1]\n",
    "    submission = pd.DataFrame({'id': test_id, 'song_popularity': stacked_preds})\n",
    "    submission.to_csv('submission_stacked.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec48e20",
   "metadata": {},
   "source": [
    "Ensembling -> Normal average and Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7c32462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to submission_stacked.csv (stacked meta-model)\n"
     ]
    }
   ],
   "source": [
    "if (LGBMClassifier is not None) and (XGBClassifier is not None):\n",
    "    print(\"Predictions saved to submission_stacked.csv (stacked meta-model)\")\n",
    "else:\n",
    "    submission = pd.DataFrame({'id': test_id, 'song_popularity': ensemble})\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"Predictions saved to submission.csv (weighted ensemble)\")\n",
    "\n",
    "    # Optionally, save individual model predictions for comparison\n",
    "    pd.DataFrame({'id': test_id, 'song_popularity': test_preds_hgb}).to_csv('submission_hgb.csv', index=False)\n",
    "    if LGBMClassifier is not None:\n",
    "        pd.DataFrame({'id': test_id, 'song_popularity': test_preds_lgb}).to_csv('submission_lgb.csv', index=False)\n",
    "    if XGBClassifier is not None:\n",
    "        pd.DataFrame({'id': test_id, 'song_popularity': test_preds_xgb}).to_csv('submission_xgb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umc203",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
